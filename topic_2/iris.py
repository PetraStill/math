"""
Метод PCA автоматично знаходить нові осі координат у просторі даних, 
уздовж яких зміни найбільші. Ці нові осі називаються головними компонентами, 
і вони завжди ортогональні, тобто взаємно перпендикулярні. 
Завдяки цьому кожна компонента показує незалежний напрямок, 
не дублюючи інформацію інших.

Розглянемо приклад із класичним набором даних Iris, 
де для 150 квітів зібрано чотири числові характеристики. 
Ми зменшимо кількість вимірів із чотирьох до двох за допомогою PCA 
і перевіримо, що знайдені напрямки дійсно ортогональні.
"""
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris

# Завантажуємо дані про квіти Iris
iris = load_iris()
X = iris.data  # 150 квітів, 4 ознаки

print(f"Оригінальні дані: {X.shape}")

# Зменшуємо кількість вимірів до 2 головних компонент
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

print(f"Після PCA: {X_pca.shape}")

# Вектори головних компонент
pc1, pc2 = pca.components_
dot_product = pc1 @ pc2

print(f"\nПерша компонента: {pc1}")
print(f"Друга компонента: {pc2}")
print(f"\nСкалярний добуток PC1·PC2 = {dot_product:.10f}")
print(f"Ортогональні: {np.abs(dot_product) < 1e-10}")